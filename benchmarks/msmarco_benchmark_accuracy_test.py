# -*- coding: utf-8 -*-
"""MSMARCO benchmark accuracy test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qKP15doPnGzrcD9MWcSDuudwtEKfsfrX

# Cell 1: Setup and Installations

This cell installs necessary libraries and handles authentication for the Google Gemini API.
"""

# Cell 1: Setup & Installs (Sentence Transformers)

# Install required libraries
# sentence-transformers will pull in PyTorch or TensorFlow as needed
!pip install pandas pyarrow scikit-learn numpy fastparquet sentence-transformers --quiet

print("Libraries installed.")

# --- Moorcheh SDK Setup ---
# Assuming your SDK is not yet on PyPI, install from GitHub
# TODO: Replace with the actual URL of your SDK repository
# !pip install git+https://github.com/your-username/moorcheh-python-sdk.git --quiet
# print("Attempted Moorcheh SDK installation from GitHub.")

# For now, we will mock the Moorcheh client parts needed (binarization, ITS) later
# If the SDK is installed, you would import it in Cell 2:
# from moorcheh_sdk import MoorchehClient, MoorchehError # etc.

"""# Cell 2: Load and Prepare MS MARCO Data

This cell loads the dataset from the specified Parquet file. You'll need to upload your ms_marco_v1.1_train.parquet file to your Colab environment or mount your Google Drive where it's stored.
"""

# Cell 2: Load Data

import pandas as pd
import numpy as np
import sys # For exiting on error

# --- Configuration ---
# TODO: Adjust this path based on where you upload/mount the file
# data is downloaded from https://huggingface.co/datasets/microsoft/ms_marco/viewer?views%5B%5D=v11_train
DATASET_PATH = '/train-00000-of-00001.parquet' # Or '/train-00000-of-00001.parquet' if that's the actual name
SAMPLE_SIZE = 1000 # Process a smaller sample for faster testing, set to None for full dataset
VECTOR_DIMENSION = 768 # Dimension for all-mpnet-base-v2

# --- Load Data ---
print(f"Loading dataset from: {DATASET_PATH}")
df = None # Initialize df
try:
    # Explicitly specify the engine
    df = pd.read_parquet(DATASET_PATH, engine='pyarrow')
    print("Dataset loaded successfully.")
    print(f"Original dataset shape: {df.shape}")
except FileNotFoundError:
    print(f"ERROR: Dataset file not found at '{DATASET_PATH}'.")
    print("Please upload the file to your Colab environment or mount Google Drive and adjust the path.")
    sys.exit(1)
except ImportError as e:
     print(f"ERROR: Missing Parquet engine library: {e}")
     print("Ensure 'pyarrow' is installed (should be by Cell 1). You might need '!pip install pyarrow'.")
     sys.exit(1)
except Exception as e:
    print(f"ERROR: Failed to load Parquet file: {e}")
    if "Invalid column metadata" in str(e) or "corrupt file" in str(e):
        print("\nThis often indicates the file was corrupted during download/upload.")
        print("Recommendation: Please re-download the file from Hugging Face and upload it again.")
    sys.exit(1)

# --- Data Inspection and Preprocessing ---
print("\nDataset Columns:", df.columns.tolist())
print("\nFirst 5 rows:")
# Displaying head might be slow if 'passages' is large, consider limiting columns
# print(df.head().to_markdown(index=False))
print(df[['query_id', 'query', 'answers', 'query_type']].head().to_markdown(index=False))


required_cols = ['query', 'passages', 'query_id', 'query_type', 'answers']
missing_cols = [col for col in required_cols if col not in df.columns]
if missing_cols:
    print(f"\nERROR: Dataset is missing required columns: {missing_cols}")
    sys.exit(1)

# --- CORRECTED extract_passages_and_labels function ---
def extract_passages_and_labels(row):
    row_query_id = row.get('query_id', 'UNKNOWN_ID')
    passages_data = row.get('passages')

    if not passages_data or not isinstance(passages_data, dict):
        return [], []

    # Get NumPy arrays (or potentially lists if structure varies)
    texts_array = passages_data.get('passage_text')
    labels_array = passages_data.get('is_selected')

    # Check if they are array-like and have the same length
    # Using .size checks total elements, works for numpy arrays and lists
    if (texts_array is None or labels_array is None or
        not hasattr(texts_array, '__len__') or not hasattr(labels_array, '__len__') or
        len(texts_array) != len(labels_array) or len(texts_array) == 0):
        # print(f"DEBUG (query_id: {row_query_id}): Mismatch or invalid passage/label arrays.")
        return [], []

    filtered_texts = []
    filtered_labels = []
    # Iterate through the elements of the arrays/lists
    for i in range(len(texts_array)):
        txt = texts_array[i]
        # Check if text is a non-empty string after stripping whitespace
        if txt and isinstance(txt, str) and txt.strip():
            filtered_texts.append(txt)
            # Get corresponding label safely
            label = labels_array[i] if i < len(labels_array) else 0
            # Convert label to int, default to 0 if invalid
            try:
                filtered_labels.append(int(label))
            except (ValueError, TypeError):
                 filtered_labels.append(0)
        # else:
            # print(f"DEBUG (query_id: {row_query_id}): Skipping passage at index {i}. Invalid text.")

    return filtered_texts, filtered_labels
# --- END CORRECTED function ---


print("\nExtracting passages and relevance labels...")
# Apply the function row-wise
extracted_data = df.apply(extract_passages_and_labels, axis=1)
df['passage_texts'] = extracted_data.apply(lambda x: x[0])
df['relevance_labels'] = extracted_data.apply(lambda x: x[1])

# Filter out rows with no valid passages extracted
df_filtered = df[df['passage_texts'].apply(len) > 0].copy()

# --- Check if filtering removed all rows ---
if df_filtered.empty and not df.empty:
     print("\nERROR: After extracting passages, the filtered DataFrame is empty.")
     print("This might happen if the 'passages' column structure is different than expected,")
     print("or if all rows had empty/invalid passage texts.")
     print("Please double-check the data structure and the 'extract_passages_and_labels' function.")
     sys.exit(1)
elif df.empty:
     print("\nWarning: The original DataFrame loaded was empty.")
# --- End Check ---

print(f"Filtered dataset shape (rows with passages): {df_filtered.shape}")


# --- Sample Data (Optional) ---
if SAMPLE_SIZE is not None and SAMPLE_SIZE < len(df_filtered):
    print(f"\nSampling {SAMPLE_SIZE} rows for processing...")
    df_processed = df_filtered.sample(n=SAMPLE_SIZE, random_state=42) # Use random_state for reproducibility
else:
    df_processed = df_filtered
    print(f"\nProcessing full filtered dataset ({len(df_processed)} rows).")

print("\nSampled data head:")
print(df_processed[['query_id', 'query', 'passage_texts', 'relevance_labels']].head().to_markdown(index=False))

"""# Cell 3: Embedding Function

Defines the function to get embeddings from Gemini, including basic error handling and rate limit awareness.
"""

# Cell 3: Embedding Function (Sentence Transformers)

import time
from tqdm.notebook import tqdm # For progress bars in Colab
import numpy as np
import sys

# --- Load Sentence Transformer Model ---
# This will download the model the first time it's run in the Colab session.
# Subsequent runs will use the cached model.
try:
    from sentence_transformers import SentenceTransformer
    # Define the model name - use a high-quality, common model
    MODEL_NAME = 'all-mpnet-base-v2'
    print(f"Loading sentence-transformer model: {MODEL_NAME}...")
    # Check if GPU is available in Colab (Runtime -> Change runtime type -> Hardware accelerator -> GPU)
    # If GPU is available, sentence-transformers will use it automatically.
    # You can specify device='cuda' or device='cpu' if needed.
    model = SentenceTransformer(MODEL_NAME)
    print("Sentence transformer model loaded successfully.")
    # Verify dimension matches config (optional)
    test_emb = model.encode(["test sentence"])
    # VECTOR_DIMENSION should be defined in Cell 2
    if 'VECTOR_DIMENSION' not in globals():
         print("WARNING: VECTOR_DIMENSION not defined globally (expected from Cell 2). Using model's default.")
         VECTOR_DIMENSION = test_emb.shape[1]
    elif test_emb.shape[1] != VECTOR_DIMENSION:
         print(f"WARNING: Model dimension ({test_emb.shape[1]}) does not match configured VECTOR_DIMENSION ({VECTOR_DIMENSION}).")
         print("Please ensure VECTOR_DIMENSION in Cell 2 matches the chosen model.")
         # Update the variable for consistency if needed:
         # VECTOR_DIMENSION = test_emb.shape[1]
         # print(f"Updated VECTOR_DIMENSION to {VECTOR_DIMENSION}")

except ImportError:
    print("ERROR: sentence-transformers library not found.")
    print("Please ensure Cell 1 (Setup & Installs) ran successfully.")
    sys.exit(1)
except Exception as e:
    print(f"ERROR: Failed to load sentence-transformer model '{MODEL_NAME}': {e}")
    print("This might be due to network issues during download or model compatibility.")
    sys.exit(1)


# --- Embedding Function using Sentence Transformers ---
def get_embeddings(texts: list[str], batch_size: int = 32) -> list[np.ndarray | None]:
    """
    Embeds a list of texts using the loaded Sentence Transformer model.
    Returns a list of NumPy arrays (embeddings) or None for failed/empty inputs.
    """
    if not texts:
        return []

    # print(f"  Encoding {len(texts)} texts in batches of {batch_size}...")
    # Filter out any None or empty strings before sending to model
    valid_texts = [(i, text) for i, text in enumerate(texts) if text and isinstance(text, str) and text.strip()]
    original_indices = [item[0] for item in valid_texts]
    texts_to_encode = [item[1] for item in valid_texts]

    all_embeddings = [None] * len(texts) # Initialize with None for original length

    if not texts_to_encode:
        print("  No valid texts found to encode.")
        return all_embeddings # Return list of Nones

    try:
        # Use model.encode for batch processing - much faster than one by one
        embeddings_np = model.encode(
            texts_to_encode,
            batch_size=batch_size,
            show_progress_bar=False # Set to True for visual progress on large batches
            # convert_to_numpy=True # Default is numpy array
        )

        # Place embeddings back into the original index positions
        for i, emb in enumerate(embeddings_np):
            original_index = original_indices[i]
            all_embeddings[original_index] = emb # Store the numpy array

        # print(f"  Encoding complete. Generated {len(embeddings_np)} embeddings.")
        return all_embeddings

    except Exception as e:
        print(f"ERROR during sentence-transformer encoding: {e}")
        # Return list of Nones if batch encoding fails
        return [None] * len(texts)


# --- Test Embedding Function (Optional) ---
print("\nRunning embedding function test...")
test_texts = ["This is a test sentence.", "Another sentence for embedding.", "", None]
test_embeddings = get_embeddings(test_texts)
valid_test_embeddings = [e for e in test_embeddings if e is not None]
if valid_test_embeddings:
   print(f"Embedding test successful. Generated {len(valid_test_embeddings)} embeddings.")
   print(f"Dimension: {valid_test_embeddings[0].shape[0]}")
   print("Sample embedding:", valid_test_embeddings[0][:5])
else:
    print("\nEmbedding test failed or returned only None values.")

"""# Cell 4: Ranking & ITS Functions

This cell contains the Python functions for L2/Cosine ranking and the MIB+ITS calculations based on the code you provided earlier.
"""

# Cell 4: Ranking & ITS Functions

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
import sys # Import sys for more detailed error info
import traceback # For printing full tracebacks

# --- Binarization Function (from your Python code) ---
def create_binary_index(vector: list[float] | np.ndarray) -> np.ndarray: # Accept list or ndarray
    """Creates a binary vector based on median thresholding."""
    # Add check for None input
    if vector is None:
        print("ERROR in create_binary_index: Input vector is None.")
        return np.array([], dtype=np.uint8)

    try:
        # Ensure input is a NumPy array for calculations
        if not isinstance(vector, np.ndarray):
            vector_array = np.array(vector, dtype=np.float32)
        else:
            # If already ndarray, ensure it's float32 (median works better)
            vector_array = vector.astype(np.float32) if vector.dtype != np.float32 else vector

        if vector_array.ndim != 1:
             print(f"ERROR in create_binary_index: Input vector must be 1-dimensional. Got shape {vector_array.shape}")
             return np.array([], dtype=np.uint8)

        if vector_array.size == 0:
            # Handle empty array input
            return np.array([], dtype=np.uint8)

        median = np.median(vector_array)
        binary_index = np.where(vector_array > median, 1, 0).astype(np.uint8)
        return binary_index
    except Exception as e:
        print(f"ERROR in create_binary_index: {e}.")
        print(f"Input type: {type(vector)}, Input head: {str(vector)[:100]}...")
        traceback.print_exc() # Print full traceback for debugging
        return np.array([], dtype=np.uint8)


# --- Pairwise Counts Function (from your Python code) ---
def calculate_pairwise_counts(v1: np.ndarray, v2: np.ndarray) -> tuple[float, float, float, float]:
    """Calculates normalized contingency counts (a,b,c,d) for binary vectors."""
    # ... (Implementation remains the same as previous version with checks) ...
    if not isinstance(v1, np.ndarray) or not isinstance(v2, np.ndarray): raise TypeError(...)
    if v1.shape != v2.shape: raise ValueError(...)
    if v1.size == 0: return (0.0, 0.0, 0.0, 0.0)
    len_v = v1.size
    try:
        a = np.sum((v1 == 1) & (v2 == 1)) / len_v
        b = np.sum((v1 == 1) & (v2 == 0)) / len_v
        c = np.sum((v1 == 0) & (v2 == 1)) / len_v
        d = np.sum((v1 == 0) & (v2 == 0)) / len_v
    except Exception as e: print(f"ERROR during pairwise count calculation: {e}"); raise
    return (a, b, c, d)

# --- ITS Score Function (from your Python code) ---
def calculate_its_score(a: float, b: float, c: float, d: float) -> float:
    """Calculates the ITS score (C+ + C-) based on pairwise counts."""
    # ... (Implementation remains the same as previous version) ...
    eps = 1e-10; a=max(a,eps); b=max(b,eps); c=max(c,eps); d=max(d,eps)
    p1_dot=a+b; p0_dot=c+d; p_dot_1=a+c; p_dot_0=b+d
    term1=-p_dot_1*np.log(p_dot_1) if p_dot_1>eps else 0; term2=-p_dot_0*np.log(p_dot_0) if p_dot_0>eps else 0
    c_so=term1+term2
    c_plus_term_a=a*np.log(a/(p1_dot*p_dot_1)) if (a>eps and p1_dot>eps and p_dot_1>eps) else 0
    c_plus_term_d=d*np.log(d/(p0_dot*p_dot_0)) if (d>eps and p0_dot>eps and p_dot_0>eps) else 0
    c_plus=c_plus_term_a+c_plus_term_d
    c_minus_term_b=b*np.log(b/(p1_dot*p_dot_0)) if (b>eps and p1_dot>eps and p_dot_0>eps) else 0
    c_minus_term_c=c*np.log(c/(p0_dot*p_dot_1)) if (c>eps and p0_dot>eps and p_dot_1>eps) else 0
    c_minus=c_minus_term_b+c_minus_term_c
    if abs(c_so)>eps: its_score=(c_plus/c_so)-(c_minus/c_so)
    else: its_score=0.0
    return max(0.0, min(2.0, its_score))


# --- Ranking Functions ---
def rank_by_l2(query_embedding: np.ndarray, passage_embeddings: np.ndarray) -> list[int]:
    """Ranks passage indices by ascending L2 distance to the query."""
    # ... (implementation as before) ...
    if query_embedding is None or not isinstance(passage_embeddings, np.ndarray) or passage_embeddings.shape[0] == 0: return []
    distances = euclidean_distances(query_embedding.reshape(1, -1), passage_embeddings)[0]
    ranked_indices = np.argsort(distances)
    return ranked_indices.tolist()

def rank_by_cosine(query_embedding: np.ndarray, passage_embeddings: np.ndarray) -> list[int]:
    """Ranks passage indices by descending Cosine Similarity to the query."""
    # ... (implementation as before) ...
    if query_embedding is None or not isinstance(passage_embeddings, np.ndarray) or passage_embeddings.shape[0] == 0: return []
    similarities = cosine_similarity(query_embedding.reshape(1, -1), passage_embeddings)[0]
    ranked_indices = np.argsort(-similarities)
    return ranked_indices.tolist()

# --- REFINED rank_by_its ---
def rank_by_its(query_embedding: np.ndarray, passage_embeddings: np.ndarray) -> list[int]:
    """Ranks passage indices by descending ITS score."""
    # Initial input validation
    if query_embedding is None or not isinstance(query_embedding, np.ndarray) or query_embedding.ndim != 1:
        print(f"ERROR: Invalid query_embedding provided to rank_by_its. Type: {type(query_embedding)}, Shape: {getattr(query_embedding, 'shape', 'N/A')}")
        # Decide on fallback behavior - empty list or original order? Empty seems safer.
        return []
    if not isinstance(passage_embeddings, np.ndarray) or passage_embeddings.ndim != 2 or passage_embeddings.shape[0] == 0:
        print(f"Warning: Invalid or empty passage_embeddings provided to rank_by_its. Shape: {getattr(passage_embeddings, 'shape', 'N/A')}. Returning empty list.")
        return []

    scores = []
    passage_indices = list(range(passage_embeddings.shape[0])) # Keep track of original index

    # --- Outer try block for setup errors ---
    try:
        # Binarize query once - with added checks
        #print(f"  Binarizing query vector (shape: {query_embedding.shape})...")
        query_binary = create_binary_index(query_embedding)
        if not isinstance(query_binary, np.ndarray) or query_binary.size == 0:
             print("  ERROR: Failed to binarize query embedding. Cannot calculate ITS.")
             # If query fails, ranking is impossible, return original order or empty? Original order might be less confusing.
             return passage_indices

        #print(f"  Query binarized successfully (size: {query_binary.size}).")

        # --- Inner loop for processing each passage ---
        for i, passage_emb in enumerate(passage_embeddings):
            score = 0.0 # Default score if calculation fails for this passage
            try:
                # Check passage embedding validity before binarizing
                if passage_emb is None or not isinstance(passage_emb, np.ndarray) or passage_emb.ndim != 1:
                    print(f"  Warning: Skipping passage index {i} due to invalid embedding. Type: {type(passage_emb)}, Shape: {getattr(passage_emb, 'shape', 'N/A')}")
                    scores.append(score) # Append default score
                    continue

                # Binarize passage
                passage_binary = create_binary_index(passage_emb)
                if not isinstance(passage_binary, np.ndarray) or passage_binary.size == 0:
                    print(f"  Warning: Failed to binarize passage embedding at index {i}. Assigning score 0.")
                    scores.append(score)
                    continue

                # Check dimension match *after* successful binarization
                if query_binary.shape != passage_binary.shape:
                     print(f"  ERROR: Shape mismatch between query_binary ({query_binary.shape}) and passage_binary ({passage_binary.shape}) at index {i}. Assigning score 0.")
                     scores.append(0.0)
                     continue

                # Calculate pairwise counts
                a, b, c, d = calculate_pairwise_counts(query_binary, passage_binary)

                # Calculate ITS score
                score = calculate_its_score(a, b, c, d)
                scores.append(score)

            # Catch errors specific to processing *this* passage
            except ValueError as ve:
                 print(f"  ERROR calculating pairwise counts for passage index {i}: {ve}. Assigning score 0.")
                 scores.append(0.0)
            except TypeError as te:
                 print(f"  ERROR (TypeError) during ITS calculation for passage index {i}: {te}. Assigning score 0.")
                 scores.append(0.0)
            except Exception as e_inner:
                 print(f"  UNEXPECTED ERROR during ITS calculation for passage index {i}: {e_inner}. Assigning score 0.")
                 traceback.print_exc() # Print traceback for unexpected inner errors
                 scores.append(0.0)
        # --- End inner loop ---

        # Final check on score list length
        if len(scores) != len(passage_indices):
             print(f"ERROR: Score list length ({len(scores)}) does not match passage count ({len(passage_indices)}). Returning original order.")
             return passage_indices

        # Rank by descending ITS score using stable sort
        # print("  Sorting results by ITS score...")
        indexed_scores = list(zip(passage_indices, scores))
        indexed_scores.sort(key=lambda x: (-x[1], x[0])) # Sort descending by score, ascending by index
        ranked_indices = [index for index, score in indexed_scores]

        return ranked_indices

    except Exception as e_outer:
        # Catch errors during query binarization or final sort
        print(f"ERROR during ITS ranking setup or final sort: {e_outer}")
        traceback.print_exc() # Print traceback for outer errors
        # Fallback: return original order
        return passage_indices

print("Ranking and ITS functions defined.")

"""# Cell 5: Processing Loop

This is the main cell that iterates through the sampled data, performs embeddings and rankings, and stores the results.
"""

# Cell 5: Processing Loop (Sentence Transformers)

import time
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm # Progress bar
import sys # For error exit

# --- Storage for results ---
# List to store results for each query
evaluation_data = []

# --- Loop through sampled DataFrame ---
# Ensure df_processed exists from Cell 2
if 'df_processed' not in locals() or df_processed.empty:
     print("ERROR: Processed DataFrame 'df_processed' not found or is empty. Please run Cell 2 successfully.")
     sys.exit(1)

print(f"\nProcessing {len(df_processed)} queries...")

# Use tqdm for progress bar
for index, row in tqdm(df_processed.iterrows(), total=df_processed.shape[0], desc="Processing Queries"):
    query_id = row.get('query_id', f'unknown_index_{index}') # Use index as fallback ID
    query_text = row.get('query')
    query_type = row.get('query_type', 'UNKNOWN')
    passage_texts = row.get('passage_texts', [])
    true_relevance_labels = row.get('relevance_labels', [])

    if not query_text or not passage_texts:
        print(f"Skipping query_id {query_id}: Missing query or passages.")
        continue

    # --- Minimal logging within the loop for speed ---
    # print(f"\nProcessing Query ID: {query_id} (Type: {query_type})...")

    # --- Embed Query ---
    # get_embeddings expects a list, returns a list
    query_embedding_list = get_embeddings([query_text]) # No task type needed for sentence-transformers
    if not query_embedding_list or query_embedding_list[0] is None:
        print(f"  Failed to embed query for query_id {query_id}. Skipping.")
        continue
    # Ensure it's a NumPy array of the correct type
    query_embedding_np = np.array(query_embedding_list[0], dtype=np.float32)

    # --- Embed Passages ---
    # print(f"  Embedding {len(passage_texts)} passages...") # Reduce verbosity
    # get_embeddings returns a list of np.ndarray or None
    passage_embeddings_list = get_embeddings(passage_texts) # Batching handled inside

    # Filter out passages where embedding failed (returned None)
    valid_indices = [i for i, emb in enumerate(passage_embeddings_list) if isinstance(emb, np.ndarray)]
    if not valid_indices:
        print(f"  Failed to embed any passages for query_id {query_id}. Skipping.")
        continue

    # Create the NumPy array ONLY from valid embeddings
    passage_embeddings_np = np.array([passage_embeddings_list[i] for i in valid_indices], dtype=np.float32)
    # Keep track of original indices and corresponding labels for valid embeddings
    original_indices_map = {new_idx: old_idx for new_idx, old_idx in enumerate(valid_indices)}
    valid_relevance_labels = [true_relevance_labels[i] for i in valid_indices]
    # valid_passage_texts = [passage_texts[i] for i in valid_indices] # Keep if needed for debugging

    # print(f"  Successfully embedded {len(valid_indices)}/{len(passage_texts)} passages.") # Reduce verbosity

    # --- Calculate Rankings ---
    # print("  Calculating L2 ranking...") # Reduce verbosity
    l2_ranked_indices_new = rank_by_l2(query_embedding_np, passage_embeddings_np)
    l2_ranked_indices_orig = [original_indices_map[i] for i in l2_ranked_indices_new]

    # print("  Calculating Cosine ranking...") # Reduce verbosity
    cosine_ranked_indices_new = rank_by_cosine(query_embedding_np, passage_embeddings_np)
    cosine_ranked_indices_orig = [original_indices_map[i] for i in cosine_ranked_indices_new]

    # print("  Calculating ITS ranking...") # Reduce verbosity
    its_ranked_indices_new = rank_by_its(query_embedding_np, passage_embeddings_np)
    its_ranked_indices_orig = [original_indices_map[i] for i in its_ranked_indices_new]

    # --- Store Results ---
    evaluation_data.append({
        "query_id": query_id,
        "query_text": query_text,
        "query_type": query_type,
        "true_relevance": true_relevance_labels, # Original full relevance list
        "valid_passage_indices": valid_indices, # Indices of passages successfully embedded
        "valid_passage_relevance": valid_relevance_labels, # Relevance labels for valid passages
        "rankings": {
            "l2": l2_ranked_indices_orig,
            "cosine": cosine_ranked_indices_orig,
            "its": its_ranked_indices_orig,
        }
    })
    # print(f"  Finished processing Query ID: {query_id}") # Reduce verbosity


print(f"\nFinished processing all queries. Collected results for {len(evaluation_data)} queries.")

# Optional: Convert results to a DataFrame for easier analysis later
# eval_df = pd.DataFrame(evaluation_data)
# print("\nEvaluation DataFrame Head:")
# print(eval_df[['query_id', 'query_type']].head())

"""# Cell 6: Evaluation Metrics

This cell defines functions to calculate basic Information Retrieval metrics nDCG@10 (Normalized Discounted Cumulative Gain at cutoff 10) and MAP (Mean Average Precision).
"""

# Cell 6: Evaluation Metrics (nDCG@10 & MAP)

import numpy as np
import statistics # Using statistics for mean
from collections import defaultdict # To group data

# --- Helper Functions for Standard IR Metrics ---

def calculate_dcg_at_k(relevance_scores: list[int], k: int) -> float:
    """Calculates Discounted Cumulative Gain @ k."""
    # Use relevance scores directly (1 for relevant, 0 for irrelevant)
    # Pad with 0s if ranking has fewer than k items
    rel_k = relevance_scores[:k] + [0] * (k - len(relevance_scores))
    # DCG formula: sum(rel_i / log2(i+1)) where i is 0-based index (rank is i+1)
    gains = [(rel / np.log2(i + 2)) for i, rel in enumerate(rel_k)]
    return np.sum(gains)

def calculate_ndcg_at_k(ranked_indices: list[int], true_relevance: list[int], k: int) -> float:
    """Calculates Normalized Discounted Cumulative Gain @ k."""
    if not any(label == 1 for label in true_relevance):
        # If there are no truly relevant items, nDCG is typically considered 0 or undefined.
        # Returning 0 is common for averaging purposes.
        return 0.0

    # Get relevance scores for the ranked items
    # Handle indices out of bounds for true_relevance
    actual_relevance = [(true_relevance[idx] if idx < len(true_relevance) else 0) for idx in ranked_indices]

    # Calculate DCG for the actual ranking
    dcg = calculate_dcg_at_k(actual_relevance, k)

    # Calculate Ideal DCG (IDCG) by sorting true relevance descending
    ideal_relevance = sorted([rel for rel in true_relevance if rel > 0], reverse=True)
    idcg = calculate_dcg_at_k(ideal_relevance, k)

    # nDCG = DCG / IDCG, handle division by zero if IDCG is 0
    return (dcg / idcg) if idcg > 0 else 0.0

def calculate_average_precision(ranked_indices: list[int], true_relevance: list[int]) -> float:
    """Calculates Average Precision (AP) for a single query."""
    relevant_items_count = sum(1 for label in true_relevance if label == 1)
    if relevant_items_count == 0:
        return 0.0 # AP is 0 if no relevant items exist

    hits = 0
    sum_precisions = 0.0
    for rank_minus_1, idx in enumerate(ranked_indices):
        rank = rank_minus_1 + 1
        # Check if index is valid and item is relevant
        if idx < len(true_relevance) and true_relevance[idx] == 1:
            hits += 1
            precision_at_k = hits / rank
            sum_precisions += precision_at_k

    # Average Precision is the sum of precisions at relevant ranks divided by total relevant items
    return sum_precisions / relevant_items_count

# --- Calculate Metrics Function (Revised for nDCG@10 and MAP) ---
def calculate_metrics_by_type(eval_data: list[dict], k_for_ndcg: int = 10) -> dict:
    """
    Calculates nDCG@k and MAP for different ranking methods,
    grouped by query_type. Also includes overall metrics.
    """
    # Group evaluation data by query_type (including only queries with relevance info)
    grouped_data = defaultdict(list)
    valid_eval_data = [] # Flat list for overall calculation

    for item in eval_data:
        # We need true_relevance to calculate metrics
        if 'true_relevance' in item and isinstance(item['true_relevance'], list):
            q_type = item.get('query_type', 'UNKNOWN')
            grouped_data[q_type].append(item)
            valid_eval_data.append(item)
        else:
             print(f"Warning: Skipping query {item.get('query_id')} due to missing/invalid 'true_relevance'.")


    if not valid_eval_data:
        print("Warning: No valid queries with relevance information found in evaluation data.")
        return {} # Return empty results

    # Overall results dictionary (method -> query_type -> metrics)
    overall_results = defaultdict(lambda: defaultdict(dict))
    ranking_methods = ['l2', 'cosine', 'its']
    all_query_types = list(grouped_data.keys())

    # Calculate metrics for each query type
    for q_type, data_subset in grouped_data.items():
        print(f"  Calculating metrics for Query Type: '{q_type}' ({len(data_subset)} queries)")
        for method in ranking_methods:
            ndcg_scores = []
            ap_scores = []

            for item in data_subset:
                ranked_indices = item['rankings'].get(method, [])
                true_relevance = item['true_relevance']

                if not ranked_indices:
                    print(f"    Warning: Missing ranking for query {item.get('query_id')} method {method}. Scoring as 0.")
                    ndcg_scores.append(0.0)
                    ap_scores.append(0.0)
                    continue

                # Calculate nDCG@k
                ndcg_at_k = calculate_ndcg_at_k(ranked_indices, true_relevance, k_for_ndcg)
                ndcg_scores.append(ndcg_at_k)

                # Calculate Average Precision
                avg_precision = calculate_average_precision(ranked_indices, true_relevance)
                ap_scores.append(avg_precision)

            # Calculate mean scores for this method and query type
            method_results = {}
            method_results[f'nDCG@{k_for_ndcg}'] = statistics.mean(ndcg_scores) if ndcg_scores else 0.0
            method_results['MAP'] = statistics.mean(ap_scores) if ap_scores else 0.0
            method_results['Num_Queries'] = len(data_subset) # Use count of queries in this group
            overall_results[method][q_type] = method_results

    # Calculate overall metrics across all valid queries using the flat list
    print(f"  Calculating overall metrics ({len(valid_eval_data)} total valid queries)")
    for method in ranking_methods:
        all_ndcg_scores = []
        all_ap_scores = []
        for item in valid_eval_data: # Iterate through the pre-filtered list
             ranked_indices = item['rankings'].get(method, [])
             true_relevance = item['true_relevance']
             if not ranked_indices:
                 all_ndcg_scores.append(0.0)
                 all_ap_scores.append(0.0)
                 continue

             ndcg_at_k = calculate_ndcg_at_k(ranked_indices, true_relevance, k_for_ndcg)
             all_ndcg_scores.append(ndcg_at_k)
             avg_precision = calculate_average_precision(ranked_indices, true_relevance)
             all_ap_scores.append(avg_precision)

        # Calculate overall metrics for this method
        overall_method_results = {}
        overall_method_results[f'nDCG@{k_for_ndcg}'] = statistics.mean(all_ndcg_scores) if all_ndcg_scores else 0.0
        overall_method_results['MAP'] = statistics.mean(all_ap_scores) if all_ap_scores else 0.0
        overall_method_results['Num_Queries'] = len(valid_eval_data)
        # Score distribution is removed as it's less standard for nDCG/MAP
        overall_results[method]['overall'] = overall_method_results

    return dict(overall_results) # Convert back to standard dict

# --- Calculate Metrics ---
print("\nCalculating standard IR evaluation metrics (nDCG@10, MAP) grouped by Query Type...")
# Ensure evaluation_data (from Cell 5) is available in the scope
metrics_standard = calculate_metrics_by_type(evaluation_data, k_for_ndcg=10) # Calculate nDCG@10
print("\n--- Grouped Standard Evaluation Results (Raw) ---")
import json
print(json.dumps(metrics_standard, indent=2, default=str))
print("-----------------------------------------------")

"""# Cell 7: Display Results (Simple Table)

This cell formats the calculated metrics into a simple table.
"""

# Cell 7: Display Results (nDCG@10 & MAP)

import pandas as pd

# Use the results from the standard metrics calculation in Cell 6
# This assumes 'metrics_standard' is available
metrics_to_display_grouped = metrics_standard

# --- Define metric columns to display ---
metric_columns = ['nDCG@10', 'MAP'] # The metrics calculated in Cell 6

# --- Iterate through query types and display results ---

# Get all query types found, plus 'overall'
query_types_found = set()
for method_results in metrics_to_display_grouped.values():
    query_types_found.update(method_results.keys())

# Ensure 'overall' is last if present
display_order = sorted([qt for qt in query_types_found if qt != 'overall'])
if 'overall' in query_types_found:
    display_order.append('overall')

print("\n--- Comparison Tables (Standard IR Metrics by Query Type) ---")

for q_type in display_order:
    print(f"\n--- Query Type: {q_type.upper()} ---")
    display_data = []
    num_valid_queries_for_type = 0

    # Ensure consistent method order in the table
    for method in ['l2', 'cosine', 'its']:
        # Safely get results for this method and query type
        results = metrics_to_display_grouped.get(method, {}).get(q_type)

        if not results:
            # Handle case where a method might be missing for a type
             row = {"Ranking Method": method, "Num Queries": 0}
             for metric_col in metric_columns: row[metric_col] = "N/A" # Initialize metrics as N/A
        else:
            row = {"Ranking Method": method}
            num_valid_queries_for_type = results.get('Num_Queries', 0) # Get total valid queries for this type
            row["Num Queries"] = num_valid_queries_for_type

            # Get scores for each metric, format them
            for metric_col in metric_columns:
                 score = results.get(metric_col, 0.0) # Safely get score, default 0
                 row[metric_col] = f"{score:.4f}" # Format score

        display_data.append(row)

    # Convert to DataFrame for this query type
    metrics_df_type = pd.DataFrame(display_data)

    # Define column order for the table
    column_order = ["Ranking Method"] + metric_columns + ["Num Queries"]
    # Filter columns to only those present in the DataFrame
    existing_columns = [col for col in column_order if col in metrics_df_type.columns]
    metrics_df_type = metrics_df_type[existing_columns]

    # Print the table for this query type
    if not metrics_df_type.empty:
        print(metrics_df_type.to_markdown(index=False))
        if num_valid_queries_for_type > 0:
             print(f"(Metrics based on {num_valid_queries_for_type} valid queries of this type)")
        else:
             print("(No valid queries of this type were processed)")
    else:
        print(f"(No results calculated for query type: {q_type})")

print("\n--------------------------------------------------------------------")
print("nDCG@10: Normalized Discounted Cumulative Gain at cutoff 10.")
print("MAP: Mean Average Precision.")
print("Higher values are better for both metrics.")
# Get the overall query count from the last processed type (assuming 'overall' exists and is last)
overall_query_count = metrics_to_display_grouped.get('l2', {}).get('overall', {}).get('Num_Queries', 0)
if overall_query_count > 0:
    print(f"Overall metrics calculated over {overall_query_count} queries with relevance information.")
else:
     print("No valid queries with relevance information were processed overall.")